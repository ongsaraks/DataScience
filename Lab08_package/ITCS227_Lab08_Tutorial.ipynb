{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://www.ict.mahidol.ac.th/en/\" target=\"_blank\">\n",
    "    <img src=\"https://www3.ict.mahidol.ac.th/ICTSurveysV2/Content/image/MUICT2.png\" width=\"400\" alt=\"Faculty of ICT\">\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "# Lab08: ML Basics: Classification - Tutorial\n",
    "\n",
    "\n",
    "This tutorial will provide hands-on practice in:\n",
    "- building classifier models for binary-class and multi-class classification.\n",
    "- collecting evaluation metrics\n",
    "- using cross-validation to improve reliability of training-set / validation-set results.\n",
    "- using various types of classifier algorithms\n",
    "- approaches to dealing with imbalanced dataset problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Library of Helper Functions\n",
    "\n",
    "- This `Library of Helper Functions` is designed to help simplify many tasks when evaluating models on different datasets. Make use of these functions to save your time in trialling different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import learning_curve\n",
    "import warnings\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "\n",
    "def _show_classification_report(model, y_true, y_pred, target_names):\n",
    "    '''\n",
    "        Function to print performance metrics\n",
    "    '''\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred, pos_label=1, average='weighted')\n",
    "    specificity = recall_score(y_true, y_pred, pos_label=0, average='weighted')\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    class_report = classification_report(y_true, y_pred, target_names=target_names)\n",
    "    print(class_report)\n",
    "    res = classification_report(y_true, y_pred, target_names=target_names, output_dict=True)\n",
    "    return pd.json_normalize(res, sep='_')\n",
    "    \n",
    "def _show_confusion_matrix(model, y_true, y_pred, target_names):\n",
    "    '''\n",
    "        Function to plot confusion matrix\n",
    "    '''\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=model.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                  display_labels=target_names)\n",
    "    disp.plot(cmap=plt.cm.Blues,)\n",
    "    plt.gcf().set_size_inches(3.5, 3.5)\n",
    "    disp.ax_.set_title(f'Confusion Matrix for {model.__class__.__name__}', fontsize=8)\n",
    "    plt.show()\n",
    "    \n",
    "def _plot_histogram_of_frequencies(data, ax=None):\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(figsize=(7,2.5))\n",
    "    unique_values, counts = np.unique(data, return_counts=True)\n",
    "    barh = plt.bar(unique_values, counts)\n",
    "    plt.xlabel(\"Values\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Histogram\")\n",
    "    plt.xticks(unique_values)\n",
    "    ax.bar_label(barh, fmt='%.2f')\n",
    "    ax.set_ylim(bottom=0, top=1.25*max(counts))\n",
    "    print('Class Split:', counts/sum(counts))\n",
    "    plt.show()\n",
    "    \n",
    "def _make_learning_curve(model, X_train, y_train, scoring=\"f1_weighted\", num_training_sizes=10):\n",
    "    def _plot_learning_curve(model, train_sizes, train_scores, valid_scores, metric='F1 Score', plt_text='', ax=None):\n",
    "        if not ax:\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 3), sharey=True, sharex=True)\n",
    "        train_errors = train_scores.mean(axis=1)\n",
    "        valid_errors = valid_scores.mean(axis=1)\n",
    "        ax.plot(train_sizes, train_errors, \"r-+\", linewidth=2, label=\"train\")\n",
    "        ax.plot(train_sizes, valid_errors, \"b-\", linewidth=3, label=\"valid\")\n",
    "        ax.set_xlabel(\"Training set size\")\n",
    "        ax.set_ylabel(f'{metric}')\n",
    "        # plt.gca().set_xscale(\"log\", nonpositive='clip')\n",
    "        ax.grid()\n",
    "        ax.legend(loc=\"upper right\")\n",
    "        ax.set_ylim(bottom=0, top=1.25*max([1]))\n",
    "        ax.set_title(f'{model.__class__.__name__}\\n{plt_text}', fontsize=8)\n",
    "        plt.show()\n",
    "        \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
    "        train_sizes, train_scores, valid_scores = learning_curve( model, \n",
    "                                                    X_train, y_train, \n",
    "                                                    train_sizes=np.linspace(0.01, 1.0, num_training_sizes), # e.g. `num` size intervals, from 1% to 100%\n",
    "                                                    cv=5,     # CV=5 means  Train = 80%  , Test = 20%.\n",
    "                                                              # CV=10 means Train = 90%  , Test = 10%.\n",
    "                                                              #   - The fit/predict is repeated 5 times with random samples taken from X/Y.\n",
    "                                                              #   - The resulting error is the average across all 5 trials; so a smoother and fairer result than CV=1 , which is hold-out.\n",
    "                                                    scoring=scoring,\n",
    "                                                    n_jobs=-1\n",
    "                                                )\n",
    "    _plot_learning_curve(model, train_sizes, train_scores, valid_scores, metric=scoring.replace('_',' ').title(), plt_text='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Templates for Classification Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification Model Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "target_names = ['A','B']\n",
    "X, y = datasets.make_classification(n_samples=1000, \n",
    "                                    n_informative=4,\n",
    "                                    n_features=6, \n",
    "                                    n_classes=len(target_names),\n",
    "                                   n_clusters_per_class=len(target_names)\n",
    "                                   )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.15, random_state=0)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit( X_train, y_train )\n",
    "y_pred = model.predict( X_test )\n",
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Classification Model Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "target_names = ['A','B','C']\n",
    "X, y = datasets.make_classification(n_samples=1000, \n",
    "                                    n_informative=4,\n",
    "                                    n_features=6, \n",
    "                                    n_classes=len(target_names),\n",
    "                                   n_clusters_per_class=len(target_names)\n",
    "                                   )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.15, random_state=0)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit( X_train, y_train )\n",
    "y_pred = model.predict( X_test )\n",
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Templates for Classification Models with Metric Evaluation + Learning Curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification Model Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "target_names = ['A','B']\n",
    "X, y = datasets.make_classification(n_samples=1000, \n",
    "                                    n_informative=4,\n",
    "                                    n_features=6, \n",
    "                                    n_classes=len(target_names),\n",
    "                                   n_clusters_per_class=len(target_names)\n",
    "                                   )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.15, random_state=0)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit( X_train, y_train )\n",
    "y_pred = model.predict( X_test )\n",
    "\n",
    "_show_confusion_matrix(model, y_test, y_pred, target_names)\n",
    "_show_classification_report(model, y_test, y_pred, target_names)\n",
    "_make_learning_curve(model, X_train, y_train, scoring=\"f1_weighted\", num_training_sizes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Classification Model Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "target_names = ['A','B','C']\n",
    "X, y = datasets.make_classification(n_samples=1000, \n",
    "                                    n_informative=4,\n",
    "                                    n_features=6, \n",
    "                                    n_classes=len(target_names),\n",
    "                                   n_clusters_per_class=len(target_names)\n",
    "                                   )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.15, random_state=0)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit( X_train, y_train )\n",
    "y_pred = model.predict( X_test )\n",
    "\n",
    "_show_confusion_matrix(model, y_test, y_pred, target_names)\n",
    "_show_classification_report(model, y_test, y_pred, target_names)\n",
    "_make_learning_curve(model, X_train, y_train, scoring=\"f1_weighted\", num_training_sizes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Upgrading Hold-out to Cross Validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example K Folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, \\\n",
    "                                    StratifiedKFold, \\\n",
    "                                    StratifiedGroupKFold, \\\n",
    "                                    cross_val_score\n",
    "\n",
    "# Shuffled K-Folds:\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "scores = cross_val_score(model, X_train, y_train, \n",
    "                         cv=kf, \n",
    "                         scoring='f1_weighted')\n",
    "\n",
    "# Shuffled Stratified K-Folds by Class:\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "scores = cross_val_score(model, X_train, y_train, \n",
    "                         cv=skf, \n",
    "                         scoring='f1_weighted')\n",
    "\n",
    "# # Shuffled Stratified K-Folds by Group:\n",
    "groups = [ 1,1,2,2,2,... ] # Group per X_train record.\n",
    "groups = [0]*len(X_train)\n",
    "gkf = StratifiedGroupKFold(n_splits=5, shuffle=True)\n",
    "# scores = cross_val_score(model, X_train, y_train, \n",
    "#                          groups=groups, cv=gkf, \n",
    "#                          scoring='f1_weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example implementation code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, \\\n",
    "                                    StratifiedKFold, \\\n",
    "                                    GroupKFold, \\\n",
    "                                    cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "target_names = ['A','B']\n",
    "X, y = datasets.make_classification(n_samples=1000, \n",
    "                                    n_informative=4,\n",
    "                                    n_features=6, \n",
    "                                    n_classes=len(target_names),\n",
    "                                   n_clusters_per_class=len(target_names)\n",
    "                                   )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.15, random_state=0)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "scoring='f1_weighted'\n",
    "\n",
    "# K-Fold Cross-Validation on the Training Set:\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "# Perform cross validation on the training data:\n",
    "train_f1_scores = cross_val_score(model, X_train, y_train, cv=kf, scoring=scoring)\n",
    "\n",
    "print( f'Cross Validation {scoring} Scores: {train_f1_scores}')\n",
    "print(f'Mean: {np.mean(train_f1_scores):.4f}')\n",
    "print(f'Std:  {np.std(train_f1_scores):.4f}')\n",
    "\n",
    "\n",
    "# Proceed to fit on full training set & evaluate on test set:\n",
    "model.fit( X_train, y_train )\n",
    "y_pred = model.predict( X_test )\n",
    "\n",
    "_show_confusion_matrix(model, y_test, y_pred, target_names)\n",
    "_show_classification_report(model, y_test, y_pred, target_names)\n",
    "_make_learning_curve(model, X_train, y_train, scoring=scoring, num_training_sizes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Many Classifier Algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67aTiUcVsPhq",
    "papermill": {
     "duration": 0.012546,
     "end_time": "2025-02-26T14:13:58.017917",
     "exception": false,
     "start_time": "2025-02-26T14:13:58.005371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### LogisticRegression Model:\n",
    "\n",
    "✅ **Logistic Regression** provides **probabilistic predictions** and performs well when features are correlated. **Logistic Regression often performs better**, making it a preferred choice for structured numerical datasets like loan approvals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6L0RwJ0HXG4",
    "outputId": "23fa9a91-e639-40dd-d143-459943e42037",
    "papermill": {
     "duration": 0.078445,
     "end_time": "2025-02-26T14:13:58.149169",
     "exception": false,
     "start_time": "2025-02-26T14:13:58.070724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67aTiUcVsPhq",
    "papermill": {
     "duration": 0.012546,
     "end_time": "2025-02-26T14:13:58.017917",
     "exception": false,
     "start_time": "2025-02-26T14:13:58.005371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Naive Bayes Model:\n",
    "\n",
    "✅ **Naïve Bayes** is a simple, fast, and computationally efficient classifier, especially effective for text-based problems. However, its **independence assumption** means that each feature does not influence another feature; and this may not always hold in real-world data, which can impact accuracy.\n",
    "- Despite Naïve Bayes' limitations, its **mathematical simplicity** and **interpretability** make it a strong choice in many applications, including **spam detection** and **text classification**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nAYHlqzsJFO",
    "papermill": {
     "duration": 0.027944,
     "end_time": "2025-02-26T14:13:58.058226",
     "exception": false,
     "start_time": "2025-02-26T14:13:58.030282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# model = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear (Stochastic Gradient Descent) Classifier Model:\n",
    "\n",
    "✅ **SGDClassifier** works on a similar premise as **LogisticRegression**, however it allows for partial fitting to iteratively control its learning behaviour (sample-by-sample) and enabling scalability on large datasets. This is unavailable in LogisticRegression, which learns by batch. As SGD requires careful tuning, standard LogisticRegression is often preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# model = SGDClassifier(loss=\"log_loss\", random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67aTiUcVsPhq",
    "papermill": {
     "duration": 0.012546,
     "end_time": "2025-02-26T14:13:58.017917",
     "exception": false,
     "start_time": "2025-02-26T14:13:58.005371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### DecisionTree Model:\n",
    "\n",
    "✅ Decision Trees are highly interpretable classifiers, capable of capturing non-linear relationships and handling both categorical and numerical data. However, they are susceptible to overfitting, particularly with complex trees, and can create unstable models if the training data is slightly altered.\n",
    "\n",
    "Decision Trees' intuitive visualization and ability to provide explicit decision rules make them valuable for tasks like feature importance analysis and applications where understanding the decision-making process is crucial, such as medical diagnosis and risk assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67aTiUcVsPhq",
    "papermill": {
     "duration": 0.012546,
     "end_time": "2025-02-26T14:13:58.017917",
     "exception": false,
     "start_time": "2025-02-26T14:13:58.005371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### RandomForest Model:\n",
    "\n",
    "✅ Random Forests are `ensemble` classifiers (consisting of many Decision Trees). They often give high accuracy and even on medium-complexity datasets. However, they can be less interpretable than single decision trees and do require more computational resources for training and prediction, especially with a large number of trees. To start, we will use 100 trees (`n_estimators=100`) and all available CPU cores to train them (`n_jobs=-1`).\n",
    "\n",
    "Random Forests are more resistance to overfitting and have an ability to provide feature importance estimates. They are useful to test and you can often expect high accuracy and overall generalized fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# model = RandomForestClassifier(n_estimators=100, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67aTiUcVsPhq",
    "papermill": {
     "duration": 0.012546,
     "end_time": "2025-02-26T14:13:58.017917",
     "exception": false,
     "start_time": "2025-02-26T14:13:58.005371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Support Vector Machine Model:\n",
    "✅ Support Vector Machines (SVMs) are `sophisticated` classifiers, for linear and `non-linear` data problems by using their transformation (`kernel`) function. \n",
    "\n",
    "Although computationally expensive for large datasets, they usually give a `generalized fit` and can be very accurate if you can **find the right transformation function** (kernel, parameters and regularization parameter), which can be challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "\n",
    "# model = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 - Experimenting with many models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "target_names = ['A','B']\n",
    "X, y = datasets.make_classification(n_samples=1000, \n",
    "                                    n_informative=4,\n",
    "                                    n_features=6, \n",
    "                                    n_classes=len(target_names),\n",
    "                                    n_clusters_per_class=len(target_names), \n",
    "                                    random_state=0\n",
    "                                   )\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def get_models(): \n",
    "    class_weights = dict(zip(np.unique(y_train), \n",
    "    compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)     ))\n",
    "    return [   \n",
    "        LogisticRegression( class_weight='balanced', solver='lbfgs' ),\n",
    "        # SGDClassifier(loss=\"log_loss\", random_state=0, class_weight=class_weights ),\n",
    "        # GaussianNB(),\n",
    "        # DecisionTreeClassifier(class_weight='balanced' ),\n",
    "        # RandomForestClassifier(n_estimators=100, class_weight='balanced_subsample', n_jobs=-1),\n",
    "        # SVC( C=1.0, kernel='rbf', degree=3, gamma='scale', class_weight='balanced' ),\n",
    "        # SVC( C=1.0, kernel='linear', class_weight='balanced' ),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = get_models()\n",
    "for model in models:\n",
    "    print(model.__class__.__name__)\n",
    "    model.fit( X_train, y_train )\n",
    "    y_pred = model.predict( X_test )\n",
    "    _show_confusion_matrix(model, y_test, y_pred, target_names)\n",
    "    _show_classification_report(model, y_test, y_pred, target_names)\n",
    "    _make_learning_curve(model, X_train, y_train, scoring=\"f1_weighted\", num_training_sizes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6 - Templates for Classification Models with Imbalanced Datasets:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Check Imbalance:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "target_names = ['A','B']\n",
    "X, y = datasets.make_classification(n_samples=1000, \n",
    "                                    n_informative=4,\n",
    "                                    n_features=6, \n",
    "                                    n_classes=len(target_names),\n",
    "                                    n_clusters_per_class=len(target_names), \n",
    "                                    weights=[0.75,0.25],\n",
    "                                    random_state=0\n",
    "                                   )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.15, random_state=0)\n",
    "\n",
    "_plot_histogram_of_frequencies(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `Stratify by y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "target_names = ['A','B']\n",
    "X, y = datasets.make_classification(n_samples=1000, \n",
    "                                    n_informative=4,\n",
    "                                    n_features=6, \n",
    "                                    n_classes=len(target_names),\n",
    "                                    n_clusters_per_class=len(target_names), \n",
    "                                    weights=[0.75,0.25],\n",
    "                                    random_state=0\n",
    "                                   )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.15, random_state=0, stratify=y)\n",
    "\n",
    "_plot_histogram_of_frequencies(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `class_weight='balanced'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "target_names = ['A','B']\n",
    "X, y = datasets.make_classification(n_samples=1000, \n",
    "                                    n_informative=4,\n",
    "                                    n_features=6, \n",
    "                                    n_classes=len(target_names),\n",
    "                                    n_clusters_per_class=len(target_names), \n",
    "                                    weights=[0.75,0.25],\n",
    "                                    random_state=0\n",
    "                                   )\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.15, random_state=0)\n",
    "_plot_histogram_of_frequencies(y_train)\n",
    "\n",
    "\n",
    "# Cost-Sensitive Logistic Regression Using class_weight='balanced'\n",
    "# Initialize Logistic Regression with class_weight='balanced'\n",
    "model = LogisticRegression(solver='liblinear', \n",
    "                           class_weight='balanced', \n",
    "                           random_state=0)\n",
    "\n",
    "\n",
    "model.fit( X_train, y_train )\n",
    "y_pred = model.predict( X_test )\n",
    "_show_confusion_matrix(model, y_test, y_pred, target_names)\n",
    "_show_classification_report(model, y_test, y_pred, target_names)\n",
    "_make_learning_curve(model, X_train, y_train, scoring=\"f1_weighted\", num_training_sizes=10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `Undersampling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "target_names = ['A','B']\n",
    "X, y = datasets.make_classification(n_samples=1000, \n",
    "                                    n_informative=4,\n",
    "                                    n_features=6, \n",
    "                                    n_classes=len(target_names),\n",
    "                                    n_clusters_per_class=len(target_names), \n",
    "                                    weights=[0.75,0.25],\n",
    "                                    random_state=0\n",
    "                                   )\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.15, random_state=0)\n",
    "_plot_histogram_of_frequencies(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_undersampled( X_train, y_train, target_column_name='target' ):\n",
    "    X_train_cp = pd.DataFrame(X_train.copy())\n",
    "    X_train_cp[target_column_name] = y_train\n",
    "    y_0 = X_train_cp[X_train_cp[target_column_name] == 0]\n",
    "    y_1 = X_train_cp[X_train_cp[target_column_name] == 1]\n",
    "    y_0_undersample = y_0.sample(y_1.shape[0])\n",
    "    df_train_undersampled = pd.concat([y_0_undersample, y_1], axis = 0)\n",
    "    return df_train_undersampled\n",
    "\n",
    "target_name = 'target'\n",
    "df_train_undersampled = get_undersampled( X_train, y_train, target_column_name=target_name )\n",
    "X_train_under = df_train_undersampled.drop(columns=[target_name])\n",
    "y_train_under = df_train_undersampled[target_name]\n",
    "\n",
    "_plot_histogram_of_frequencies( y_train_under )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit( X_train_under, y_train_under )\n",
    "y_pred = model.predict( X_test )\n",
    "_show_confusion_matrix(model, y_test, y_pred, target_names)\n",
    "_show_classification_report(model, y_test, y_pred, target_names)\n",
    "_make_learning_curve(model, X_train, y_train, scoring=\"f1_weighted\", num_training_sizes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `Oversampling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "target_names = ['A','B']\n",
    "X, y = datasets.make_classification(n_samples=1000, \n",
    "                                    n_informative=4,\n",
    "                                    n_features=6, \n",
    "                                    n_classes=len(target_names),\n",
    "                                    n_clusters_per_class=len(target_names), \n",
    "                                    weights=[0.75,0.25],\n",
    "                                    random_state=0\n",
    "                                   )\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.15, random_state=0)\n",
    "_plot_histogram_of_frequencies(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "\n",
    "print('Original dataset shape %s' % Counter(y_train))\n",
    "sm = SVMSMOTE(random_state=42)\n",
    "X_train_SMOTE, y_train_SMOTE = sm.fit_resample(X_train, y_train)\n",
    "print('Resampled dataset shape %s' % Counter(y_train_SMOTE))\n",
    "_plot_histogram_of_frequencies( y_train_SMOTE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit( X_train_SMOTE, y_train_SMOTE )\n",
    "y_pred = model.predict( X_test )\n",
    "_show_confusion_matrix(model, y_test, y_pred, target_names)\n",
    "_show_classification_report(model, y_test, y_pred, target_names)\n",
    "_make_learning_curve(model, X_train, y_train, scoring=\"f1_weighted\", num_training_sizes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `Curriculum Learning`\r\n",
    "**Theory:**\n",
    "\n",
    "Curriculum learning for imbalanced datasets involves training a model progressively, starting with simpler training data representations and moving to more difficulty samples to improve model performance. In theory, the model is first exposed to simplified, balanced versions of the data. As the model progresses, the data's complexity is gradually increased, introducing more realistic imbalances and challenging cases. This staged approach allows the model to learn fundamental patterns without being overwhelmed by the initial imbalance. By focusing on easier examples first, the model develops a robust understanding of the minority class. This hopefully leads to better generalization and improved performance on the imbalanced dataset.\n",
    "\n",
    "This original method was inspired by the general concept of curriculum learning and published here:\n",
    "> Reference:  **Yoshua Bengio et al. on curriculum learning is foundational: Bengio, Y., Louradour, J., Collobert, R., & Weston, J. (2009). Curriculum learning. Proceedings of the 26th annual international conference on machine learning, 1  41-48. [https://dl.acm.org/doi/10.1145/1553374.1553380](https://dl.acm.org/doi/10.1145/1553374.1553380)**\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "In our implementation below requires the use of the `model.partial_fit( ... )` method. Not all classifiers have this, so we can try two:\n",
    "\n",
    "\n",
    "| Model Type         | Model Class                      | Supports `partial_fit`? |\n",
    "| ------------------ | -------------------------------- | ----------------------- |\n",
    "| **Linear Models**  | `SGDClassifier`                  | ✅ Yes                  |\n",
    "| **Naive Bayes**    | `GaussianNB`                     | ✅ Yes                  |\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. We use an **\"easy\" dataset** created through `undersampling the majority class`, providing a balanced, albeit reduced, training set. This allows the model to initially learn fundamental patterns without the bias introduced by severe class imbalance.\n",
    "2. Subsequently, a **\"medium\" dataset**, generated using `SMOTE-SVM oversampling`, introduces synthetic minority class instances, refining the model's ability to recognize these crucial patterns.\n",
    "3. The **\"hard\" dataset** is the **original imbalanced data**, used for fine-tuning from the prior training rounds. This staged approach aims to improve model generalization and performance on the minority class.    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target_names = ['A','B']\n",
    "X, y = datasets.make_classification(n_samples=1000, \n",
    "                                    n_informative=4,\n",
    "                                    n_features=6, \n",
    "                                    n_classes=len(target_names),\n",
    "                                    n_clusters_per_class=len(target_names), \n",
    "                                    weights=[0.75,0.25],\n",
    "                                    random_state=0\n",
    "                                   )\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.15, random_state=0)\n",
    "_plot_histogram_of_frequencies(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fit model on imbalanced data as baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model = SGDClassifier(loss=\"log_loss\", random_state=0)\n",
    "\n",
    "model.fit( X_train, y_train )\n",
    "y_pred = model.predict( X_test )\n",
    "\n",
    "_show_confusion_matrix(model, y_test, y_pred, target_names)\n",
    "_show_classification_report(model, y_test, y_pred, target_names)\n",
    "_make_learning_curve(model, X_train, y_train, scoring=\"f1_weighted\", num_training_sizes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def curriculum_learning_partial_fit( model, X_train, y_train, X_test, y_test,\n",
    "                                    xy_train=[(X_train_under, y_train_under),\n",
    "                                              (X_train_SMOTE, y_train_SMOTE),\n",
    "                                              (X_train, y_train)], \n",
    "                                    complexity_levels=[0.2, 0.5, 1.0] ):\n",
    "    scaler = StandardScaler()\n",
    "    _ = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    for (_x, _y) in xy_train:\n",
    "        _x_scaled = scaler.transform(_x)\n",
    "        for level in complexity_levels:\n",
    "            subset_size = int(level * len(_x))\n",
    "            X_curr, y_curr = _x_scaled[:subset_size], _y[:subset_size]\n",
    "            try:\n",
    "                model.partial_fit(X_curr, y_curr, classes=np.unique(_y))\n",
    "            except Exception as e:\n",
    "                print(model.__class__.__name__, 'was unable to call .partial_fit( .. ).\\nAborting.' )\n",
    "                return None\n",
    "    \n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    _show_classification_report(model, y_test, y_pred, target_names)\n",
    "    _show_confusion_matrix(model, y_test, y_pred, target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "curriculum_learning_partial_fit( model, X_train, y_train, X_test, y_test,\n",
    "                                xy_train=[(X_train_under, y_train_under),\n",
    "                                          (X_train_SMOTE, y_train_SMOTE),\n",
    "                                          (X_train, y_train)], \n",
    "                                complexity_levels=[0.2, 0.5, 1.0] )\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
