{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a44ee3ed",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://www.ict.mahidol.ac.th/en/\" target=\"_blank\">\n",
    "    <img src=\"https://www3.ict.mahidol.ac.th/ICTSurveysV2/Content/image/MUICT2.png\" width=\"400\" alt=\"Faculty of ICT\">\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "# Tutorial 12: Computer Vision with Pretrained Models\n",
    "\n",
    "This tutorial provides a hands-on introduction to computer vision using pre-trained models.  It covers two key tasks: image classification (identifying the main object in an image) and object detection (locating and classifying multiple objects within an image).  Leveraging the power of transfer learning, we'll utilize pre-trained models (like EfficientNet or YOLOv8) to achieve high accuracy without extensive training.  Finally, the tutorial demonstrates how to deploy these computer vision models as interactive web applications using Streamlit, making them easily accessible and shareable.  No prior deep learning expertise is assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ef5799",
   "metadata": {},
   "source": [
    "### Download the dataset:\n",
    "1. Download this lab's dataset from here: [LAB12-DATASETS](https://studentmahidolac-my.sharepoint.com/:u:/g/personal/suppawong_tua_mahidol_ac_th/ERH9Og_vxLRLnvuFTxgo8s0BYN7s4pN1tJxuvn8DQch3BA?e=CRwRsN)\n",
    "2. Extract it under drive D: (to make sure there is no spaces in the path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0f0e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the data path\n",
    "#It is recommended that you extract \"datasets\" folder in D:/ to avoid having \"white spaces\" in the path\n",
    "data_path = 'lab12_datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7307d8f1",
   "metadata": {},
   "source": [
    "## Exercise 01: Image Classification\n",
    "This tutorial demonstrates image classification using EfficientNet (a pre-trained model) in PyTorch. We'll use a tiny \"Cats vs. Dogs\" dataset for simplicity.  Topics include model selection,preprocessing, feature extraction, fine-tuning, evaluation, and saving the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e25f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image  # For checking image loading\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08135acd",
   "metadata": {},
   "source": [
    "### 1. Dataset Loading and Preprocessing\n",
    "We'll use a tiny \"Cats vs. Dogs\" dataset due to possible limitation of computational resources. The data should be organized into folders: train/cat, train/dog, val/cat, val/dog, test/cat, test/dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c7b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = data_path+'/cats_dogs_tiny' # Replace with your data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3244a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the data directory exists\n",
    "if not os.path.exists(data_dir):\n",
    "    print(f\"Error: Data directory '{data_dir}' not found. Please download and organize the dataset.\")\n",
    "    exit() # Stop execution if the directory doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d187c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations (important for pre-trained models)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),  # Resize and crop for EfficientNet\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([ # Validation transforms (no random augmentations)\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([ # Test transforms (no random augmentations)\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc45aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data using ImageFolder (easier with organized folders)\n",
    "trainset = torchvision.datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "valset = torchvision.datasets.ImageFolder(root=os.path.join(data_dir, 'val'), transform=transform_val)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=os.path.join(data_dir, 'test'), transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = trainset.classes # Get the class names directly from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116ac1b1",
   "metadata": {},
   "source": [
    "### 2. Choosing a Pre-trained Model (EfficientNet)\n",
    "We'll use EfficientNet-b0.  EfficientNet is known for its good performance and efficiency. Other options: ResNet, MobileNet (for mobile).  Consider accuracy vs. speed trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c430fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.efficientnet_b0(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d19646c",
   "metadata": {},
   "source": [
    "### 3. Modifying the Classifier (Fully Connected Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47635eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNet's classifier is designed for 1000 ImageNet classes. We need to change it.\n",
    "num_features = model.classifier[1].in_features  # Access the correct layer for EfficientNet\n",
    "model.classifier[1] = nn.Linear(num_features, len(classes))  # Replace with our classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f570785",
   "metadata": {},
   "source": [
    "### 4. Training the Model (Fine-tuning)\n",
    "**Warning**: This step will take a while (~5 minutes) if you run on CPU, even with a tiny training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e2b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if CUDA is available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c03ed47",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gifly\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\gifly\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1458\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1458\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gifly\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1420\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1416\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1419\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1420\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1421\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1422\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\gifly\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1239\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1251\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1255\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1256\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gifly\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[1;32mc:\\Users\\gifly\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\connection.py:256\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gifly\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\connection.py:329\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    327\u001b[0m             _winapi\u001b[38;5;241m.\u001b[39mPeekNamedPipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\gifly\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\connection.py:878\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    875\u001b[0m                 ready_objects\u001b[38;5;241m.\u001b[39madd(o)\n\u001b[0;32m    876\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 878\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m \u001b[43m_exhaustive_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaithandle_to_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32mc:\\Users\\gifly\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\connection.py:810\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    808\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m--> 810\u001b[0m     res \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mWaitForMultipleObjects(L, \u001b[38;5;28;01mFalse\u001b[39;00m, timeout)\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam optimizer often works well\n",
    "\n",
    "num_epochs = 5  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {running_loss/len(trainloader)}, Val Loss: {val_loss/len(valloader)}, Val Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3906b35b",
   "metadata": {},
   "source": [
    "### 5. Model Evaluation (on the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762445a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:  # Iterate over the validation set\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(f\"Accuracy of the model on the test images: {100 * correct / total}%\")\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "cr = classification_report(y_true, y_pred, target_names=classes)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(cr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b27f4",
   "metadata": {},
   "source": [
    "### 6. Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d41196",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cats_dogs_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab3fb9d",
   "metadata": {},
   "source": [
    "### 7. To load the model (example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d1c59b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.efficientnet_b0(pretrained=False)\n",
    "num_features = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_features, len(classes))\n",
    "model.load_state_dict(torch.load('cats_dogs_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4843b45e",
   "metadata": {},
   "source": [
    "## Exercise 02: Object Detection\n",
    "This exercise demonstrates object detection using YOLOv8 (a pre-trained model) in PyTorch. We'll use a small publicly available dataset for simplicity.  The focus is on *using* the model, not building it from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aca4d3",
   "metadata": {},
   "source": [
    "### 1. Install ultralytics (if you haven't already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab6b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba865e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import cv2  # For image processing\n",
    "import matplotlib.pyplot as plt  # For displaying images (optional)\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f737c3a5",
   "metadata": {},
   "source": [
    "### 2. Load a Pre-trained YOLOv8 Model \n",
    "YOLOv8 offers different sizes (n, s, m, l, x).  'n' is the smallest and fastest, good for this tutorial. You can choose a larger size if you have more resources and need higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')  # Load the small YOLOv8 model (pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c15a12",
   "metadata": {},
   "source": [
    "### 3. Prepare a Tiny Dataset (Example: Fruits)\n",
    "\n",
    "For this exercise, let's use a tiny dataset of images with fruits (e.g., apples, bananas). The images should be in a folder (e.g., 'data/fruits_tiny/images').  Annotations (bounding boxes and labels) should be in a corresponding folder (e.g., 'data/fruits_tiny/labels') in YOLO format (txt files).\n",
    "```\n",
    "Example directory structure:\n",
    "fruits_tiny/\n",
    "  images/\n",
    "    apple1.jpg\n",
    "    banana1.jpg\n",
    "    ...\n",
    "  labels/\n",
    "    apple1.txt (contains bounding box coordinates and class label)\n",
    "    banana1.txt\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c524c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = data_path+'/fruits_tiny/train'  # Replace with your dataset directory\n",
    "sample_data_dir = data_path+'/fruits_tiny/sample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff579e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the data directory exists\n",
    "#if not os.path.exists(data_dir):\n",
    "#    print(f\"Error: Data directory '{data_dir}' not found. Please download and organize the dataset.\")\n",
    "#    exit() # Stop execution if the directory doesn't exist\n",
    "\n",
    "#image_dir = os.path.join(data_dir, 'images')\n",
    "\n",
    "if not os.path.exists(sample_data_dir):\n",
    "    print(f\"Error: Data directory '{sample_data_dir}' not found. Please download and organize the dataset.\")\n",
    "    exit() # Stop execution if the directory doesn't exist\n",
    "    \n",
    "sample_image_dir = os.path.join(sample_data_dir, 'images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f60764",
   "metadata": {},
   "source": [
    "### 4. Perform Object Detection on a Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e93667",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(sample_image_dir, '01_ Apple.jpg')  # Example image path\n",
    "results = model(image_path)  # Run object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aebc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the results\n",
    "for result in results:  # Loop through detected objects\n",
    "    boxes = result.boxes  # Bounding boxes object\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box.xyxy[0]  # Bounding box coordinates (top-left, bottom-right)\n",
    "        cls = int(box.cls[0])  # Class ID\n",
    "        conf = box.conf[0]  # Confidence score\n",
    "\n",
    "        # Draw bounding box and label on the image (using OpenCV)\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Correct color format for matplotlib\n",
    "        cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2) # Green box\n",
    "        cv2.putText(img, f'{result.names[cls]} {conf:.2f}', (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # (Optional) Display the image with bounding boxes using matplotlib\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Detected Objects in {os.path.basename(image_path)}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa6ac8",
   "metadata": {},
   "source": [
    "### # 5. Object Detection on a Batch of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65271657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also perform object detection on a batch of images.\n",
    "\n",
    "image_files = [os.path.join(sample_image_dir, f) for f in os.listdir(sample_image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))] # Get all image files\n",
    "\n",
    "results_batch = model(image_files) # Pass a list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0745a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the results for each image in the batch (similar to the single image example)\n",
    "for i, result in enumerate(results_batch):\n",
    "    boxes = result.boxes  # Bounding boxes object\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box.xyxy[0]  # Bounding box coordinates (top-left, bottom-right)\n",
    "        cls = int(box.cls[0])  # Class ID\n",
    "        conf = box.conf[0]  # Confidence score\n",
    "\n",
    "        # Draw bounding box and label on the image (using OpenCV)\n",
    "        img = cv2.imread(image_files[i])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Correct color format for matplotlib\n",
    "        cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2) # Green box\n",
    "        cv2.putText(img, f'{result.names[cls]} {conf:.2f}', (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # (Optional) Display the image with bounding boxes using matplotlib\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Detected Objects in {os.path.basename(image_files[i])}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0edafc4",
   "metadata": {},
   "source": [
    "*That is why you should fine-tune the model with your dataset if you can!* However, for this tutorial, we will simply use the pre-trained model without fine-tuning it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72efbdaf",
   "metadata": {},
   "source": [
    "### 6. Model Evaluation (on a validation set)\n",
    "YOLOv8 provides built-in evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bad1ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assuming you have a validation set (e.g., 'fruits_data/val'), you can evaluate the model like this:\n",
    "results_val = model.val(data=data_path+'/fruits_tiny/fruits_tiny.yaml') # Where fruits_data.yaml is your data configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb27f0",
   "metadata": {},
   "source": [
    "### 7. Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e6cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is already saved when you load it, but if you fine-tune it, you'll want to save the updated weights.\n",
    "# model.save('fine_tuned_yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6f6df",
   "metadata": {},
   "source": [
    "### 8. Exporting to ONNX (for Streamlit Deployment)\n",
    "For deployment with Streamlit, exporting to ONNX format is often recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77216a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "success = model.export(format='onnx')  # Export the model to ONNX format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ef055f",
   "metadata": {},
   "source": [
    "## Exercise 03 (Optional): Image Segmentation\n",
    "This exercise demonstrates image segmentation using a pre-trained DeepLabv3 model in PyTorch. We'll use the PASCAL VOC 2012 dataset (or a subset) for demonstration.  We'll focus on *using* the model for inference, not training or fine-tuning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66c93b1",
   "metadata": {},
   "source": [
    "### 1. Install Required Libraries (if you haven't already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c7f635",
   "metadata": {},
   "source": [
    "### 2. Load the Pre-trained DeepLabv3 Model\n",
    "We'll use a DeepLabv3 model pre-trained on the PASCAL VOC dataset.  Other options include U-Net, FCN, etc. Segmentation-models-pytorch is an excellent library for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce04ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "ENCODER = 'efficientnet-b7' # You can try other encoders like 'resnet50', 'efficientnet-b0', etc.\n",
    "ENCODER_WEIGHTS = 'imagenet' # Or None if you want to train from scratch\n",
    "CLASSES = 21 # Number of classes in PASCAL VOC (20 foreground + 1 background)\n",
    "\n",
    "model = smp.DeepLabV3(\n",
    "    encoder_name=ENCODER,\n",
    "    encoder_weights=ENCODER_WEIGHTS,\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "model.eval() # Set the model to evaluation mode (important for inference)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Use GPU if available\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33afa50d",
   "metadata": {},
   "source": [
    "### 3. Prepare the Image and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f69fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image\n",
    "image_path = data_path+'/seg_example/boat.jpg'  \n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"Error: Image path '{image_path}' not found.\")\n",
    "    exit()\n",
    "    \n",
    "img = Image.open(image_path).convert(\"RGB\") # Open and convert to RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c6a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations (important for pre-trained models)\n",
    "# These should match how the model was trained\n",
    "transform = T.Compose([\n",
    "    T.Resize([256,344]),  # Resize the image\n",
    "    T.ToTensor(),  # Convert to tensor\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Normalize using ImageNet stats\n",
    "])\n",
    "\n",
    "input_tensor = transform(img).unsqueeze(0)  # Add a batch dimension (models expect batches)\n",
    "input_tensor = input_tensor.to(device) # Move tensor to device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15544174",
   "metadata": {},
   "source": [
    "### 4. Make Predictions (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d71b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Disable gradient calculations for inference\n",
    "    output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c96f6c7",
   "metadata": {},
   "source": [
    "### 5. Post-processing and Visualization\n",
    "The output is a tensor containing the segmentation mask (pixel-wise class predictions). Post-processing is usually needed to get a nice segmented image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9317b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted segmentation mask (the class with the highest probability for each pixel)\n",
    "mask = torch.argmax(output, dim=1).squeeze().cpu().numpy() # Get the predicted mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c23c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the mask to a color image (optional, for better visualization)\n",
    "# You'll need a colormap that maps class indices to colors.\n",
    "# Here's a simple example for PASCAL VOC:\n",
    "VOC_COLORMAP = np.array([[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n",
    "                        [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n",
    "                        [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n",
    "                        [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n",
    "                        [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n",
    "                        [0, 64, 128]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1814c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_image = VOC_COLORMAP[mask].astype(np.uint8) # Create the segmented image from the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2076b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original and segmented images\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img)\n",
    "plt.title(\"Original Image\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(segmented_image)\n",
    "plt.title(\"Segmented Image\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529083f",
   "metadata": {},
   "source": [
    "### 6. Saving the Model\n",
    "It's best practice to save only the model's state dictionary (the learned weights). This makes the saved file smaller and more portable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c897115",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'deeplabv3_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac8c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model in Streamlit:\n",
    "# model = smp.DeepLabv3(encoder_name=ENCODER, encoder_weights=None, classes=CLASSES) # Create model instance\n",
    "# model.load_state_dict(torch.load('deeplabv3_model.pth')) # Load saved weights\n",
    "# model.eval() # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3908343",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">That's it! Congratulations! <br> \n",
    "    Let's now work on your lab assigment.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
