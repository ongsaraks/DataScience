{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bca3098",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://www.ict.mahidol.ac.th/en/\" target=\"_blank\">\n",
    "    <img src=\"https://www3.ict.mahidol.ac.th/ICTSurveysV2/Content/image/MUICT2.png\" width=\"400\" alt=\"Faculty of ICT\">\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "# Lab12: Computer Vision with Pretrained Models\n",
    "\n",
    "This lab assignment provides a practical introduction to image classification using pre-trained deep learning models with PyTorch. You will work with the EfficientNet-b0 model, a powerful and efficient architecture, and apply it to a small \"Ants vs. Bees\" dataset. The lab focuses on two key transfer learning techniques: feature extraction and fine-tuning. You will learn how to load and modify pre-trained models, preprocess image data, train and evaluate your models, and compare the performance of feature extraction versus fine-tuning. The lab also includes saving the trained models for potential deployment.\n",
    "\n",
    "Upon completion of this lab, you will be able to:\n",
    "\n",
    "1. **Load and Modify Pre-trained Models**: Load a pre-trained EfficientNet-b0 model from torchvision and modify its classifier layer to adapt it to a new dataset.\n",
    "2. **Implement Feature Extraction**: Freeze the pre-trained model's layers and train only the newly added classifier for feature extraction.\n",
    "3. **Perform Fine-tuning**: Unfreeze and train some or all of the pre-trained model's layers along with the new classifier for fine-tuning.\n",
    "4. **Preprocess Image Data**: Apply necessary image transformations (resizing, normalization, data augmentation) for pre-trained models.\n",
    "5. **Train and Evaluate Models**: Train the feature extraction and fine-tuning models using PyTorch and evaluate their performance using metrics like accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "6. **Compare Model Performance**: Analyze and compare the performance of the feature extraction and fine-tuning models, discussing the differences in results.\n",
    "7. **Save Trained Models**: Save the trained models for later use, such as deployment in a Streamlit application.\n",
    "8. **Understand Transfer Learning**: Gain a practical understanding of transfer learning concepts, including the trade-offs between feature extraction and fine-tuning.\n",
    "9. **Deploy the classification model**: Gain practical experience on deploying the fine-tuned model with a simple Streamlit application.\n",
    "\n",
    "\n",
    "__Intructions:__\n",
    "1. Append your ID at the end of this jupyter file name. For example, ```ITCS227_Lab12_Assignment_6788123.ipynb```\n",
    "2. Complete each task in the lab.\n",
    "3. Once finished, raise your hand to call a TA.\n",
    "4. The TA will check your work and give you an appropriate score.\n",
    "5. Submit the source code to MyCourse as record-keeping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76527d89",
   "metadata": {},
   "source": [
    "## Task01: Classification Model Development\n",
    "\n",
    "In this lab, we will use the \"Ants vs. Bees\" dataset, available as part of the lab package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96df0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the path to the dataset. \n",
    "data_dir = 'lab12_datasets\\hymenoptera_data'    #<-- Change it to the actual path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befdd3d7",
   "metadata": {},
   "source": [
    "###  1. Setup and Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e19df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db515c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the data directory exists\n",
    "if not os.path.exists(data_dir):\n",
    "    print(f\"Error: Data directory '{data_dir}' not found. Please download and organize the dataset.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b245d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b290ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "trainset = torchvision.datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=os.path.join(data_dir, 'val'), transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = trainset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b8c4af",
   "metadata": {},
   "source": [
    "### 2. Load Pre-trained EfficientNet-b0 (Feature Extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8eff6",
   "metadata": {},
   "source": [
    "Load the pre-trained EfficientNet-b0 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de190f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature_extraction = torchvision.models.efficientnet_b0(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79db6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers (feature extraction)\n",
    "for param in model_feature_extraction.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the classifier\n",
    "num_features = model_feature_extraction.classifier[1].in_features\n",
    "model_feature_extraction.classifier[1] = nn.Linear(num_features, len(classes))\n",
    "\n",
    "# Move the model to the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_feature_extraction.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3595ee7",
   "metadata": {},
   "source": [
    "### 3. Train the Feature Extraction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19682aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the hyperparameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_feature_extraction = optim.Adam(model_feature_extraction.classifier[1].parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3377e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer_feature_extraction.zero_grad()\n",
    "        outputs = model_feature_extraction(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_feature_extraction.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Feature Extraction Loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5d7893",
   "metadata": {},
   "source": [
    "### 4. Evaluation of Feature Extraction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e50c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "y_true_fe = []\n",
    "y_pred_fe = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model_feature_extraction(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        y_true_fe.extend(labels.cpu().numpy())\n",
    "        y_pred_fe.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(f\"Accuracy of Feature Extraction model: {100 * correct / total}%\")\n",
    "print(classification_report(y_true_fe, y_pred_fe, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4483b16b",
   "metadata": {},
   "source": [
    "### 5. Load Pre-trained EfficientNet-b0 (Fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af019fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine_tuning = torchvision.models.efficientnet_b0(pretrained=True)\n",
    "num_features = model_fine_tuning.classifier[1].in_features\n",
    "model_fine_tuning.classifier[1] = nn.Linear(num_features, len(classes))\n",
    "model_fine_tuning.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c3c79",
   "metadata": {},
   "source": [
    "### 6. Train the Fine-tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd40615",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_fine_tuning = optim.Adam(model_fine_tuning.parameters(), lr=0.0001) # Lower learning rate for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff4c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the num_epochs as needed. This cell can take several minutes with CPU.\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer_fine_tuning.zero_grad()\n",
    "        outputs = model_fine_tuning(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_fine_tuning.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Fine-tuning Loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48dba0e",
   "metadata": {},
   "source": [
    "### 7. Evaluation of Fine-tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ea804",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "y_true_ft = []\n",
    "y_pred_ft = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model_fine_tuning(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        y_true_ft.extend(labels.cpu().numpy())\n",
    "        y_pred_ft.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(f\"Accuracy of Fine-tuning model: {100 * correct / total}%\")\n",
    "print(classification_report(y_true_ft, y_pred_ft, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e5dbb",
   "metadata": {},
   "source": [
    "### 8. Comparison of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43592733",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparison of Models:\")\n",
    "print(\"Feature Extraction Model:\")\n",
    "print(classification_report(y_true_fe, y_pred_fe, target_names=classes))\n",
    "print(\"Fine-tuning Model:\")\n",
    "print(classification_report(y_true_ft, y_pred_ft, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476b2e3",
   "metadata": {},
   "source": [
    "### 9. Save the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8106b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_feature_extraction.state_dict(), 'ants_bees_feature_extraction.pth')\n",
    "torch.save(model_fine_tuning.state_dict(), 'ants_bees_fine_tuning.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a1fc8",
   "metadata": {},
   "source": [
    "### 10. Answer the following questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f9e044",
   "metadata": {},
   "source": [
    "**Q1**: *What is the primary difference between feature extraction and fine-tuning in the context of transfer learning?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf71872",
   "metadata": {},
   "source": [
    "A1: Fine tuning is better than Feature Extraction because is study on added datasets which make F1 score bettwe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f057d43",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:red\">&#x1F6C8; Help</span> (Use this only as a last resort!!)</summary>\n",
    "    \n",
    "```\n",
    "A1: Feature extraction involves freezing the pre-trained model's layers and training only a newly added classifier on top of the pre-trained features. Fine-tuning, on the other hand, involves unfreezing some or all of the pre-trained model's layers and training them along with the new classifier, allowing the model to adapt its learned features to the specific task.\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e446e9",
   "metadata": {},
   "source": [
    "**Q2**: *Why is it important to use the same image transformations during inference (evaluation) as were used during training?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b283342",
   "metadata": {},
   "source": [
    "A2: To ensure that input data is in the same format and does not make bias to accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137cf970",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:red\">&#x1F6C8; Help</span> (Use this only as a last resort!!)</summary>\n",
    "    \n",
    "```\n",
    "A2: Pre-trained models are trained on data that has been preprocessed in a specific way. Using the same transformations during inference ensures that the input data is in the same format and distribution as the data the model was trained on, leading to consistent and accurate predictions.\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f9251",
   "metadata": {},
   "source": [
    "**Q3**: *What are the advantages of using a pre-trained model like EfficientNet-b0 for image classification, compared to training a model from scratch?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c839b2eb",
   "metadata": {},
   "source": [
    "A3: Reducing training time and Resource consumtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df51805a",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:red\">&#x1F6C8; Help</span> (Use this only as a last resort!!)</summary>\n",
    "    \n",
    "```\n",
    "A3: Using a pre-trained model offers several advantages:\n",
    "* Reduced training time: The model has already learned general image features.\n",
    "* Less data required: Fine-tuning or feature extraction often requires significantly less data than training from scratch.\n",
    "* Improved performance: Pre-trained models often achieve higher accuracy due to the rich feature representations learned from large datasets.\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5de4fa",
   "metadata": {},
   "source": [
    "**Q4**: *What is the purpose of freezing the pre-trained model's layers when performing feature extraction?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1dbc6",
   "metadata": {},
   "source": [
    "A4: To prevent weight from being updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdca1d5b",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:red\">&#x1F6C8; Help</span> (Use this only as a last resort!!)</summary>\n",
    "    \n",
    "```\n",
    "A4: Freezing the pre-trained model's layers prevents their weights from being updated during training. This ensures that the learned features from the pre-trained model are preserved and used as fixed feature extractors.\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9835bedc",
   "metadata": {},
   "source": [
    "**Q5**: *What metrics are used to evaluate the performance of the image classification models in this lab?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d374d4",
   "metadata": {},
   "source": [
    "A5: Accuracy, precision, recall, F1-score, Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db617ba",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:red\">&#x1F6C8; Help</span> (Use this only as a last resort!!)</summary>\n",
    "    \n",
    "```\n",
    "A5: The models are evaluated using accuracy, precision, recall, F1-score, and a confusion matrix. These metrics provide a comprehensive understanding of the model's performance in terms of overall correctness, class-specific performance, and potential misclassifications.\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf7b1e1",
   "metadata": {},
   "source": [
    "**Q6**: *What is the purpose of the torch.save() function in the lab, and what information is saved?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137bce1",
   "metadata": {},
   "source": [
    "A6: To save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db80ae49",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:red\">&#x1F6C8; Help</span> (Use this only as a last resort!!)</summary>\n",
    "    \n",
    "```\n",
    "A6: The torch.save() function is used to save the trained models' state dictionaries (the learned weights and biases). This allows the models to be loaded and used later for inference or deployment without needing to retrain them.\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f36e20",
   "metadata": {},
   "source": [
    "**Q7**: *Why do we add the unsqueeze(0) when using a single image for inference?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c72c3e",
   "metadata": {},
   "source": [
    "A7: To make input in the correct format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed2d090",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:red\">&#x1F6C8; Help</span> (Use this only as a last resort!!)</summary>\n",
    "    \n",
    "```\n",
    "A7: Most PyTorch models, especially those for computer vision, expect input data in batches (even if it's a batch of size 1). unsqueeze(0) adds a batch dimension to the image tensor, making it compatible with the model's input format.\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f687dd1",
   "metadata": {},
   "source": [
    "**Q8**: *What is the difference between `.eval()` and `.train()` in PyTorch?* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904de871",
   "metadata": {},
   "source": [
    "A8: evaluation mode and training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa86b8",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:red\">&#x1F6C8; Help</span> (Use this only as a last resort!!)</summary>\n",
    "    \n",
    "```\n",
    "A8: .eval() sets the model to evaluation mode, which turns off features like dropout and batch normalization that are used during training. .train() sets the model to training mode, enabling these features. It's crucial to use .eval() during inference to ensure consistent predictions.\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c6cac",
   "metadata": {},
   "source": [
    "**Q9**: *In the lab, which model (feature extraction or fine-tuned) achieved higher accuracy on the validation dataset, and why might this be the case?* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3551864e",
   "metadata": {},
   "source": [
    "A9: fine-tuning because it's studied on added datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f0fa5a",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:red\">&#x1F6C8; Help</span> (Use this only as a last resort!!)</summary>\n",
    "    \n",
    "```\n",
    "A9: Typically, the fine-tuned model achieves higher accuracy. This is because fine-tuning allows the model to adapt the pre-trained weights to the specific characteristics of the \"Ants vs. Bees\" dataset, leading to more specialized and accurate feature representations.\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cc4707",
   "metadata": {},
   "source": [
    "**Q10**: *If you had a much larger dataset of ants and bees images, how might that change the performance difference between feature extraction and fine-tuning?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed0939",
   "metadata": {},
   "source": [
    "A10: fine tuning will be better because it get more data to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09182e3",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:red\">&#x1F6C8; Help</span> (Use this only as a last resort!!)</summary>\n",
    "    \n",
    "```\n",
    "A10: With a larger dataset, fine-tuning would likely show a more significant improvement in performance compared to feature extraction. The larger dataset would provide enough data for the model to effectively adapt its pre-trained weights without overfitting, leading to more accurate and robust feature representations.\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd6862",
   "metadata": {},
   "source": [
    "## Task02: Deploy Your Best Image Classification Model with Streamlit\n",
    "\n",
    "1. **Select Your Best Model**: Determine which model (`ants_bees_fine_tuning.pth` or `ants_bees_feature_extraction.pth`) achieved the highest validation accuracy during the lab.\n",
    "\n",
    "2. **Create a Streamlit App**:\n",
    "    - Write a Python script (`app.py`) using Streamlit.\n",
    "    - Load the selected model's state dictionary (`.pth` file). Remember to define the EfficientNet-b0 model architecture in your script.\n",
    "    - Implement an image upload functionality using `st.file_uploader()`.\n",
    "    - Apply the same image preprocessing transformations (resizing, normalization) used during training.\n",
    "    - Perform inference on the uploaded image using your loaded model.\n",
    "    - Display the predicted class (ants or bees) and the corresponding confidence score.\n",
    "    - Optionally, display the uploaded image and the class probabilities.\n",
    "\n",
    "3. **Use the Tutorial as a Guide**: You can use the \"Cats vs. Dogs Image Classification\" Streamlit tutorial provided earlier as a template. Adapt the code to load your \"Ants vs. Bees\" model and display the appropriate results.\n",
    "\n",
    "4. **Run Your App**: Run your Streamlit app from the command line using streamlit run app.py.\n",
    "\n",
    "5. **Test Your App**: Upload various ant and bee images to test the performance of your deployed model.\n",
    "\n",
    "### Deliverables\n",
    "Along with this Notebook file, please submit the followings:\n",
    "* The app.py script containing your Streamlit application.\n",
    "* A screenshot of your running Streamlit application displaying a successful prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77449f05",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:red\">&#x1F6C8; Help</span> (Use this only as a last resort!!)</summary>\n",
    "    \n",
    "```python\n",
    "# app.py - Streamlit app for Ants vs. Bees image classification\n",
    "\n",
    "import streamlit as st\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Load the Fine-tuned Model\n",
    "\n",
    "# Load the saved model (make sure the model definition is available in your Streamlit app)\n",
    "# You need to have the same model architecture definition as in your training script.\n",
    "# For example, if you trained with EfficientNet-B0:\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:main', 'efficientnet_b0', pretrained=False)\n",
    "num_features = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_features, 2)  # 2 classes (ants, bees)\n",
    "model.load_state_dict(torch.load('ants_bees_fine_tuning.pth', map_location=torch.device('cpu'))) # Load to CPU\n",
    "model.eval()\n",
    "\n",
    "# Define the image transformations (same as in training/validation)\n",
    "transform = T.Compose([\n",
    "    T.Resize(256),       # Resize for EfficientNet\n",
    "    T.CenterCrop(224),   # Center crop for consistent input size\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet stats\n",
    "])\n",
    "\n",
    "classes = ['ants', 'bees']  # Class names (same as in training)\n",
    "\n",
    "# 2. Create the Streamlit App\n",
    "\n",
    "st.title(\"Ants vs. Bees Image Classifier\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    image = Image.open(uploaded_file).convert(\"RGB\")\n",
    "    st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
    "\n",
    "    if st.button(\"Classify\"):\n",
    "        with st.spinner(\"Classifying...\"):  # Show a spinner while processing\n",
    "            input_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                probabilities = torch.nn.functional.softmax(output[0], dim=0) # Softmax for probabilities\n",
    "                predicted_class_index = torch.argmax(probabilities).item()\n",
    "                predicted_class = classes[predicted_class_index]\n",
    "                confidence = probabilities[predicted_class_index].item() * 100\n",
    "\n",
    "            st.header(\"Prediction\")\n",
    "            st.write(f\"The image is a {predicted_class} with {confidence:.2f}% confidence.\")\n",
    "\n",
    "            # Display probabilities for each class (optional)\n",
    "            st.subheader(\"Class Probabilities\")\n",
    "            for i, class_name in enumerate(classes):\n",
    "              st.write(f\"{class_name}: {probabilities[i].item()*100:.2f}%\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7f5a16",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">That's it! Congratulations! <br> \n",
    "    Now, call an LA to check your solution. Then, upload your code on MyCourses.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
